Methodology
System Overview and Threat Model
The overall system involves a content creator (the artist) who produces a digital image and an authentication authority which could simply be the artist themselves or a platform that vouches for the artist. The artist has a pair of cryptographic keys: a private key (kept secret) and a public key (widely shared or certified by a digital certificate). The private key will be used to create digital signatures. We assume the artist’s public key is known to verifiers (this could be via a public blockchain registry of artist keys, a certificate authority, or embedding the public key fingerprint in the artwork metadata for reference).
The process has two main phases:
Embedding Phase (Signing and Watermarking):
 The artist takes the final version of their artwork image (let us denote the image as I) and computes a cryptographic hash of it. The hash function can be a standard one like SHA-256, yielding a digest
			H = SHA-256(I).
 Next, the artist uses their private key to sign this hash, producing a digital signature S. The signature could be produced by an algorithm like RSA, ECDSA, or any secure signature scheme. For concreteness, imagine RSA is used, so:
					 S = H(I)^d mod N
 (in RSA notation), where (N, d) is the private key. Or, if using ECDSA, S would be a pair of integers (r, s) computed from H and the private key through elliptic curve operations. The specifics can vary, but in all cases S is a piece of binary data that proves knowledge of the private key and is tied to the image’s content hash. Now, this signature S (or some representation of it) is embedded into the image itself to create a stego-image I′. The embedding should not significantly alter the visual appearance of I. The output I′ is then released or distributed. Optionally, the artist might also publish the original hash or other information, but it should not be necessary — I′ will be self-contained for verification given the public key.
Verification Phase: A viewer or a validation tool takes the suspect image (supposedly an artwork by the artist) and extracts the hidden signature bits from it, recovering S* (the extracted signature) and perhaps other data like an extracted hash. The verifier then computes the hash of the received image (or certain features of it, depending on scheme) – call this H*. Using the artist’s public key, the verifier checks the signature S* against H*. If the signature is valid (meaning it was created by the corresponding private key and matches the image’s hash), then the image is authentic (untampered and indeed signed by the artist). If the check fails, the image is either a forgery or has been altered since it was signed. This process provides a clear yes/no answer on authenticity.
Threat Model: We assume the primary threat is a forger who wants to create or alter artwork and pass it off as an original by the artist. The forger might employ an AI model to generate an image in the artist’s style or might take a genuine signed image and manipulate it (e.g., insert new elements or combine parts of different images). The forger does not possess the artist’s private key (that is kept secure). Therefore, they cannot create a valid signature for any new content. They may attempt to copy a signature from one image to another or remove it entirely. Copying a signature from image A to image B will fail verification because the signature corresponds to A’s hash, not B’s. Removing or randomizing the signature bits will cause verification to fail as well (and likely can be noticed as an authentication failure). Thus, the security of the scheme relies on the cryptographic hardness of forging signatures (which for RSA/ECDSA is well-established) and the difficulty of altering the steganographic payload without detection.
Digital Signature Generation
We first detail the signature generation process mathematically. Let the original artwork image be represented as a sequence of bytesI = (b₁, b₂, ..., bₙ), where n is the file size in bytes or the number of relevant data points. For simplicity, one can consider n = width × height × color channels for raw pixel data, but if the image is compressed, n would be the file length. We apply a cryptographic hash function H(.) to I. We assume H produces an output of ℓ bits (e.g., ℓ = 256 for SHA-256). This gives us a message digest:
d = H(I)
where d is an ℓ-bit number representing the image content. This hash function should be collision-resistant (to prevent two different images from yielding the same d) and ideally behaves like a random oracle.
Next, using the artist’s private key, we generate a signature on d. The specifics depend on the chosen signature scheme:
If RSA is used: The private key is (N, e, dₚᵣᵢᵥ) where N is the modulus, e the public exponent, and dₚᵣᵢᵥ the private exponent. The signature could be computed as:
		S = dᵈₚᵣᵢᵥ mod N
However, in practice RSA signatures are computed on a padded hash (using schemes like PSS or PKCS#1 v1.5), so d would first be formatted with a padding scheme. Conceptually, RSA raises the hash to the secret exponent to produce the signature number S. S will be an integer modulo N (which has a size equal to the key length, e.g., 2048 bits).
If ECDSA is used: The private key is a large integer $x$ (the secret scalar), and the public key is $Q The private key is a large integer x (the secret scalar), and the public key is Q = x · G (point multiplication of generator G on the elliptic curve). The signature is a pair (r, s) computed by generating a random per-signature k, computing a point R = kG (let Rₓ be its x-coordinate), and then r = Rₓ mod n (where n is the curve’s order). Then,
s = k⁻¹(H(I) + x · r) mod n
The pair (r, s) is the signature. Verification involves checking that
s⁻¹(H(I) · G + r · Q)
equals R on the curve, which implies r matches.
Regardless of the scheme, the end result is a signature as a sequence of bits. We denote the signature bits as S₍bᵢₜₛ₎. For RSA, S₍bᵢₜₛ₎ is the binary representation of the large integer S. For ECDSA, S₍bᵢₜₛ₎ is the concatenation of the binary representations of r and s. In our design, we might choose a specific scheme based on desired signature size: ECDSA with a 256-bit curve yields a 512-bit signature (two 256-bit numbers). RSA can be shorter or longer depending on key size (a 1024-bit RSA key yields a ~1024-bit signature, which is 128 bytes; 2048-bit yields 256 bytes). Newer schemes like Ed25519 (signature size 512 bits) could also be used. For illustration, we target 256-bit security, which typically results in a 512-bit signature. Embedding a 512-bit payload in an image is quite feasible with minimal visual impact, as we will show.
Steganographic Embedding of the Signature
With the signature bits in hand, we proceed to embed them in the image. The embedding algorithm must satisfy:
Imperceptibility: The introduced changes to the image should be unnoticeable under normal viewing conditions.
Deterministic Extraction: The verifier should be able to extract the exact bit string from the image reliably.
Capacity: The image must have enough “space” to hide the signature data.
Fragility (or specificity): Any change to the image that is not an exact original should cause the extracted bits to differ, making verification fail. (We do not necessarily need to detect where the change is, just that authenticity is lost. If localization is desired, more complex block-wise schemes can be used, but at a minimum the scheme should not falsely accept a tampered image.)
Choice of Domain: We consider two main choices – embedding in the spatial domain vs. frequency domain. Spatial domain (like simple LSB substitution) is easier and has high capacity, but is very sensitive to modifications. Frequency domain (like DCT coefficients) can be more robust to compression, but since our signature is tied to exact content, robustness to content-altering operations isn’t needed (we want to detect content change). However, robustness to some extent is needed for operations that are not content changes but potential distribution steps – for example, converting an image from PNG to JPEG is not an attempt to forge, but it will change pixel values slightly. If our scheme is too fragile, even that conversion will break the signature and yield false forgery alarms. We might thus seek a semi-fragile approach: allow certain transformations (like a moderate JPEG compression) while still verifying authenticity. This can be achieved either by embedding in a robust manner or by designing the signature to be on invariant features. For now, we will embed in spatial domain under the assumption that the images will be distributed in a format that preserves exact pixel values (or that artists will provide the authentic version for verification). In Section 5 (Discussion) we will revisit the case of lossy transformations.
LSB Embedding: The simplest method is to overwrite the least significant bits of selected pixels with the signature bits. For a color image (24 bits per pixel, 8 per channel), each pixel has 3 LSBs (one per channel) that can be used, assuming we change only the lowest bit of each channel. Changing an LSB of a color channel will alter the color value by at most 1 unit out of 256, which is generally imperceptible amidst typical image noise or texture. The human eye cannot discern a 1-level change in a single color component in most cases. If the image has large flat areas of solid color, many flipped LSBs might introduce very slight speckle, but generally it’s negligible.
Let m be the message bit string we want to embed (in our case, m = S₍bᵢₜₛ₎). Let |m| = L bits. We need to choose L pixel channel positions to embed into. The positions can be predetermined or calculated via a key-dependent pseudo-random sequence for security, to make it harder for an attacker to locate the bits. A common approach is to use a pseudo-random number generator (PRNG) seeded with a secret key to generate a permutation of all pixel indices, and take the first L of them as embedding positions. However, since we want verifiers (who may not share a secret key) to extract the message, we might simply choose a known scanning order, such as row-by-row, for embedding. This is less secure if an attacker attempts to target the watermark, but since the signature is cryptographically protected, the worst outcome is randomization of those bits, which would break the signature and render the image inauthentic—precisely what we aim to detect. To improve robustness, we could include a small redundancy or error-detecting code in the signature bits. This would ensure that any random changes to the payload are caught as a verification failure, rather than coincidentally producing a valid signature pattern.
For now, assume we embed starting from the top-left pixel going row-major, one channel at a time (skipping perhaps the alpha channel if present). The embedding algorithm can be described as:
Embedding Algorithm (LSB example):
Input: Original image I (pixel array), signature bit string S_bits of length L.
Output: Stego-image I' with S_bits embedded.
1. Let I' = I (make a copy to modify).
2. Determine a sequence of L pixel channel indices to embed into.
   For example, for i from 1 to L:
       choose pixel (x, y) and color channel c for bit i.
       (This could be (x,y,c) = ((i mod width), (i/width), (i mod 3)) as a simple mapping, 
        or a pseudorandom sequence.)
3. For each bit position i from 1 to L:
       Let b = S_bits[i] (the i-th bit of the signature).
       Locate the corresponding pixel (x,y) and channel c for this i.
       Set the LSB of I'[x,y,c] to b. 
       (That is, if I'[x,y,c] was an 8-bit value, we do: I'[x,y,c] = (I'[x,y,c] & 0xFE) | b.)
4. Return I'.
In pseudocode, we might simply iterate through pixels until we place all bits. A refinement is to scatter the bits across the image rather than clustering them in the first pixels, to minimize any localized distortion. But with 512 bits total, even clustering in one area likely won’t be visible; nonetheless, distributing uniformly is a good practice.
After embedding, the stego-image I′ contains the signature in its LSBs. The difference I′ − I in pixel values is at most 1 for the modified channels. We can analyze the distortion introduced using the Mean Squared Error (MSE) between the original and the stego-image, and from that compute the Peak Signal-to-Noise Ratio (PSNR).
For an image of size M × N (pixels) with 8 bits per channel, the MSE is calculated as:
 	MSE = (1 / (M × N × 3)) × Σₓ₌₁ᴹ Σᵧ₌₁ᴺ Σ_{c ∈ {R,G,B}} (I[x, y, c] − I′[x, y, c])²
Since we only change the least significant bits, each altered channel contributes at most (±1)² = 1 to the sum, and only in the channels where bits are embedded. If we embed into L channels, and assuming the worst case that each embedded bit flips the existing LSB (whereas on average only half need changing), the number of changed values is approximately L / 2. So we can approximate:
	MSE = (L / 2) / (M × N × 3)
For example, consider a 1920 × 1080 image (which has M × N = 2,073,600 pixels). If we embed 512 bits and assume around 256 changes:
 	MSE = 256 / (2,073,600 × 3) ≈ 4.11 × 10⁻⁵
This value is extremely low. The corresponding PSNR (Peak Signal-to-Noise Ratio) is given by:
PSNR = 10 × log₁₀((MAX²) / MSE),  where MAX is the maximum possible pixel value difference (255 for 8-bit images).
Plugging in our MSE:
	 PSNR = 10 × log₁₀(255² / 4.11 × 10⁻⁵) => 10 × log₁₀(6.5 × 10⁸) => 10 × 8.81 = 88.1 dB
This is an extraordinarily high PSNR value, indicating virtually no visual distortion. For reference, a PSNR of 40 dB is typically considered excellent, while 30 dB is moderate. Even if we embed a much larger payload—say, a 16 KB signature (which is unnecessarily large, but useful as a thought experiment)—this would mean L = 131072 bits, with approximately 65536 changes assuming half need flipping. In the same 1920 × 1080 image, the MSE would rise to approximately 0.0105, and the resulting PSNR would be around 68 dB, still far above the visibility threshold. Thus, in terms of capacity, hiding a few hundred bytes (several thousand bits) is very safe in a large image. Noroozi et al. (2015) similarly reported a high PSNR of approximately 54 dB after embedding a 10 KB message into a 124 KB image. Other methods they cited commonly achieved PSNR values in the 30–40 dB range even at significantly larger payloads. On the other hand, LSB substitution meets our imperceptibility criterion easily for small signatures. We should still avoid embedding in extremely sensitive areas if possible – e.g., if an image has large uniform black background, flipping LSBs there might cause tiny speckles in the darkness (though 1/256 intensity difference in black is still black to human eyes due to display quantization). Some techniques choose not to embed in very dark or very bright areas, or use the second LSB in such areas for caution. We can incorporate a simple rule: skip embedding in pixels where the value is 0 or 255 (pure black or white) since a flip would stand out more (0->1 or 255->254 might be just barely noticeable on certain displays). But this is usually overkill.
Alternative embedding schemes: If a more robust embedding is desired (to survive e.g. JPEG compression), one might use DCT-based embedding. For example, divide the image into 8×8 blocks, take DCT of each, and in each block tweak a mid-frequency coefficient (like the one at position (4,3) in the zigzag order) up or down to encode a bit (e.g., make it even for 0, odd for 1). Many classic watermarking schemes do this. Verification would involve extracting those coefficients and checking the parity or threshold. However, integrating that with a cryptographic signature is a bit more complex because any allowed change (like JPEG) will slightly alter DCT coefficients and could throw off the embedded bits. Robust watermark schemes often integrate error correction coding to recover from bit flips. In our design, to keep it tractable, we assume either the distribution format is lossless or that the artist will provide the image in a format that preserves the bits. If needed, we could use a compromise: embed the signature in multiple redundant copies across the image so that even if some parts get destroyed, others survive. This is akin to spread-spectrum watermarking – one could spread the signature’s bits out as a pseudorandom pattern of slight changes over many pixels, making it harder to remove without heavy image degradation (Tirkel et al., 1996). For example, a technique might add a very faint textured pattern to the image that encodes the signature when correlating with a known key pattern. This can be robust to minor filtering or compression. But the trade-off is complexity and the need for a key to detect (which might complicate public verification).
Given our scenario (fine art images which can be shared in PNG or high-quality JPEG), we expect that typically the exact image can be obtained for verification (e.g., an art marketplace or gallery could keep the lossless original for checking). Therefore, our core method uses simple spatial embedding.
Ensuring Synchronization: One challenge in steganography is that the extractor needs to know where the bits were embedded. If any permutation or secret ordering was used, the same must be known to the verifier. In a public verification setting, we likely avoid using a secret key for the embedding positions (otherwise every verifier would need that key). Instead, we could derive positions from a deterministic function of the image (content-based) or just use a fixed known pattern (like starting top-left). A content-based method was exemplified by the PCNN approach – but that’s quite advanced. A simpler approach uses the image’s own hash to seed a PRNG for positions. Since the verifier can compute the image hash anyway, they can regenerate the sequence. However, if the image is tampered, the hash changes and then the wrong positions might be checked, so that complicates things – likely the verification would just fail because nothing sensible would be extracted. It might actually serve as an additional obfuscation and an attacker who doesn’t know the original hash would not know exactly where bits were embedded. But it also makes partial tampering detection harder.
For clarity, we will assume a simple known scanning (which is what our pseudocode above did). In practice, this is a point for enhancement: one could choose a pattern that maximizes spread and avoids low-entropy areas, possibly using a fixed PRNG seed known to all (like using the artist’s public key as a seed to pick positions – so it’s consistent for that artist’s works).
Verification and Extraction Procedure
The verification process is the inverse of embedding combined with signature verification.
Extraction Algorithm:
Input: Image I' suspected to contain an embedded signature, length L of expected signature (in bits).
Output: Extracted bit string S_bits (of length L).
1. Initialize empty bit string S_bits.
2. Use the same embedding positions or sequence as defined in embedding.
   For i from 1 to L:
       Identify pixel (x,y) and channel c corresponding to bit position i (same rule as embedder).
       Let b = LSB of I'[x,y,c].
       Append b to S_bits.
3. Return S_bits.
After this, we have the extracted bit string. This bit string needs to be interpreted as the signature S*. If using a straightforward scheme such as RSA, it would represent a large integer, which can be directly used for verification. If using ECDSA or a similar scheme, we need to parse the bit string into two numbers (r, s). This requires knowing how the signature was originally concatenated. For example, if the curve used is secp256k1, each of r and s is 256 bits, so the first 256 bits of S_bits represent r, and the next 256 bits represent s.
Signature Verification:
If RSA is used: We have the signer’s public key (N, e). We compute:
 d* = (S*)ᵉ mod N
 This result should equal the hash of the image if the signature is valid. In a real-world scenario, padding schemes like PKCS#1 or PSS are used, so the signature verification involves removing the padding and then comparing the hash. Typically, a cryptographic library handles this step, but conceptually, we check whether S* raised to the power e yields the expected message digest.
 Therefore, we must also compute the hash of the received image I′:
 d′ = H(I′)
Now, a subtle point arises: if the image was not altered, I' = I (the original image), and d' = d. If the image was altered in any way (including the embedded bits, but those are the alteration from I to I' we intentionally did), then in theory H(I') would differ from the original hash. Wait, this is critical: we embedded the signature into the image, so the image that the user receives (with signature) is slightly different from the original pre-embedded image. That means if we naively sign the hash of the original image and embed the signature, the image’s content changed (by the embedding) and now the hash of the stego-image is not the same as the signed hash. This seems like a paradox, but the resolution is that when we compute the signature, we actually should compute it on the image that will be distributed. In our design, I was the final image content the artist wanted to present; embedding the signature changes pixel values. However, the artist can incorporate the signature embedding as part of the creation process (i.e., accept I' as the official artwork). The changes are so minor that it doesn’t affect the visual content, so we treat 'I' as the image to be authenticated. Therefore, in the above scheme, d = H(I) should really be H(I') (the hash of the image after embedding the signature bits, since those bits are now part of the content). But that’s a circular dependency if we think in those terms (you need the image to sign it, but the image includes the signature bits which depend on the signature). How to resolve that? Typically by defining some parts of the image as not included in the hash. For example, one could zero out the locations where the signature will go, when computing the hash, so that the signature doesn’t include its own effect. Alternatively, one could iterate: guess initial bits, embed, hash, sign, re-embed until stable. A simpler method: have the signature cover only the original image content and not the portion of LSBs where the signature lives. If the image is large, the influence of flipping LSBs on the hash is negligible in a random sense, but it still changes it completely because cryptographic hash is sensitive. There’s a concept of implicit and explicit watermark: sometimes the original image hash is embedded along with the data so that you can recover original bits.
For our case, a practical solution is:
Compute hash on the original pixels, but treat the would-be watermark bits as, say, zeros during hashing.
Then sign that hash.
Embed the signature in those locations.
 Now the stego-image hash will still not match because those bits differ (original had zeros, stego has signature bits). So at verification, one must extract signature, then temporarily set those bit positions to some reference value, compute hash, verify signature. This is doable because the verifier knows where the bits are and can blank them out before hashing.
To simplify, let’s define:
Let P be the set of indices of pixels (and channels) used for embedding. 
Define a function Ĥ(I, P) that computes a hash of image I but with the pixel bits in positions P set to 0 (or some fixed value) before hashing. This effectively ignores the embedded data. The artist does: d = Ĥ(I, P) on the original image (which initially has those positions possibly with some original bits). Signs d to get S. Then embeds S in P. Now the distributed image is I' (which differs from I only in P). The verifier will extract S* from I', compute d' = Ĥ(I', P) (i.e., hash the image after zeroing out the signature bit positions). Since outside of P, I' equals the original I, and inside P we zeroed them both times, d' should equal the d the artist had. So the signature should verify against d'. This way, the signature authenticates all parts of the image except the location of the signature itself (which is fine, because those parts are just the metadata essentially).
The above method is a form of self-embedding watermark idea often used in image authentication: part of image is used to hide authentication data about the rest of the image. It creates a slight loophole in that if an attacker only tampers within those specific LSBs, it wouldn’t be caught (since we ignored them in hash). But if the attacker tampers in those LSBs, they are essentially trying to alter the signature itself – which without the private key will not result in a valid combination. So they can’t produce a meaningful alteration that still passes verification.
Thus, the verification steps are:
Extract S* from image I' (stego).
Compute d' = Ĥ(I', P) by zeroing out the signature-bit locations in I' and hashing the rest.
Use the artist’s public key to verify that S* is a valid signature on d'.
If valid, output “Authentic” (image is original and untampered); if not, output “Not authentic” or “Forgery detected”.
